{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Synthesis Parameters using word2vec\n",
    "Approach: Get \n",
    "### Current problem and solution: \n",
    "KeyError Words not in library: pass the word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained weights and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"./materials-word-embeddings/bin/word2vec_embeddings-SNAPSHOT.model\")\n",
    "model.hs = 1\n",
    "model.negative = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('total_data.pkl','rb') as f:\n",
    "    total_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.word_vec(\"QDs\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(list_):\n",
    "    # get average vector for a paragraph\n",
    "    new = np.zeros((100,))\n",
    "    count = 0\n",
    "    for word in list_:\n",
    "        try:\n",
    "            #new = np.minimum(new,new_model.word_vec(word))\n",
    "            new = np.maximum(new,new_model.word_vec(word))\n",
    "            #np.add(new,new_model.word_vec(word))\n",
    "            count += 1\n",
    "        except:\n",
    "            continue\n",
    "    return new/count\n",
    "    #return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duc/.local/lib/python3.6/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in true_divide\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "list_x = []\n",
    "list_y = []\n",
    "dataset = []\n",
    "for key in total_data.keys():\n",
    "    # fitting paper into tfidf vector\n",
    "    for paragraph in total_data[key]:\n",
    "        if paragraph[-1] == 1:\n",
    "            dataset.append(doc2vec(paragraph[0].split()))\n",
    "            if paragraph[-2] is False:\n",
    "                list_y.append(0)\n",
    "            else:\n",
    "                list_y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2731, 100)\n",
      "(2731,)\n"
     ]
    }
   ],
   "source": [
    "arr = np.asarray(dataset)\n",
    "y_train = np.asarray(list_y)\n",
    "len_train = int(len(y_train)*0.9)\n",
    "X, X_test = arr[:len_train], arr[len_train:]\n",
    "y, Y_test = y_train[:len_train], y_train[len_train:]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace Nan with 0 and inf with finite\n",
    "X = np.nan_to_num(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def over_sampler(X,y):\n",
    "    count = np.sum(y)\n",
    "    idxx = np.argwhere(y==1)[:,0]\n",
    "    new_idxx = np.random.choice(idxx, np.sum(y==0)- np.sum(y))\n",
    "    X_0 = np.concatenate([X, X[new_idxx]],axis=0)\n",
    "    y_0 = np.concatenate([y, y[new_idxx]])\n",
    "    return X_0, y_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def under_sampler(X,y):\n",
    "    idx_0 = np.argwhere(y==0)[:,0]\n",
    "    idx_1 = np.argwhere(y==1)[:,0]\n",
    "    new_idxx = np.random.choice(idx_0, np.sum(y==1))\n",
    "    y_0 = y[new_idxx]\n",
    "    X_0 = X[new_idxx]\n",
    "    y_1 = y[idx_1]\n",
    "    X_1 = X[idx_1]\n",
    "    return np.concatenate([X_0,X_1],axis=0), np.concatenate([y_0,y_1],axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4862, 100)\n",
      "(4862,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = over_sampler(X, y)\n",
    "#X_train, Y_train = under_sampler(X, y)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4862 samples\n",
      "Epoch 1/100\n",
      "4862/4862 [==============================] - 0s 67us/sample - loss: 0.6061 - acc: 0.6421\n",
      "Epoch 2/100\n",
      "4862/4862 [==============================] - 0s 54us/sample - loss: 0.5495 - acc: 0.6907\n",
      "Epoch 3/100\n",
      "4862/4862 [==============================] - 0s 58us/sample - loss: 0.5267 - acc: 0.7168\n",
      "Epoch 4/100\n",
      "4862/4862 [==============================] - 0s 58us/sample - loss: 0.5079 - acc: 0.7244\n",
      "Epoch 5/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.4971 - acc: 0.7322\n",
      "Epoch 6/100\n",
      "4862/4862 [==============================] - 0s 61us/sample - loss: 0.4876 - acc: 0.7413\n",
      "Epoch 7/100\n",
      "4862/4862 [==============================] - 0s 63us/sample - loss: 0.4821 - acc: 0.7454\n",
      "Epoch 8/100\n",
      "4862/4862 [==============================] - 0s 58us/sample - loss: 0.4657 - acc: 0.7587\n",
      "Epoch 9/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.4536 - acc: 0.7684\n",
      "Epoch 10/100\n",
      "4862/4862 [==============================] - 0s 53us/sample - loss: 0.4477 - acc: 0.7791\n",
      "Epoch 11/100\n",
      "4862/4862 [==============================] - 0s 54us/sample - loss: 0.4447 - acc: 0.7686\n",
      "Epoch 12/100\n",
      "4862/4862 [==============================] - 0s 62us/sample - loss: 0.4306 - acc: 0.7840\n",
      "Epoch 13/100\n",
      "4862/4862 [==============================] - 0s 59us/sample - loss: 0.4246 - acc: 0.7882\n",
      "Epoch 14/100\n",
      "4862/4862 [==============================] - 0s 60us/sample - loss: 0.4297 - acc: 0.7847\n",
      "Epoch 15/100\n",
      "4862/4862 [==============================] - 0s 58us/sample - loss: 0.4176 - acc: 0.7896\n",
      "Epoch 16/100\n",
      "4862/4862 [==============================] - 0s 64us/sample - loss: 0.4044 - acc: 0.7988\n",
      "Epoch 17/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.3999 - acc: 0.8075\n",
      "Epoch 18/100\n",
      "4862/4862 [==============================] - 0s 57us/sample - loss: 0.3980 - acc: 0.8044\n",
      "Epoch 19/100\n",
      "4862/4862 [==============================] - 0s 49us/sample - loss: 0.3898 - acc: 0.8077\n",
      "Epoch 20/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.3891 - acc: 0.8089\n",
      "Epoch 21/100\n",
      "4862/4862 [==============================] - 0s 47us/sample - loss: 0.3728 - acc: 0.8244\n",
      "Epoch 22/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.3676 - acc: 0.8233\n",
      "Epoch 23/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.3606 - acc: 0.8254\n",
      "Epoch 24/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.3655 - acc: 0.8229\n",
      "Epoch 25/100\n",
      "4862/4862 [==============================] - 0s 44us/sample - loss: 0.3536 - acc: 0.8385\n",
      "Epoch 26/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.3538 - acc: 0.8328\n",
      "Epoch 27/100\n",
      "4862/4862 [==============================] - 0s 55us/sample - loss: 0.3409 - acc: 0.8412\n",
      "Epoch 28/100\n",
      "4862/4862 [==============================] - 0s 44us/sample - loss: 0.3428 - acc: 0.8420\n",
      "Epoch 29/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.3334 - acc: 0.8418\n",
      "Epoch 30/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.3303 - acc: 0.8505\n",
      "Epoch 31/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.3295 - acc: 0.8497\n",
      "Epoch 32/100\n",
      "4862/4862 [==============================] - 0s 50us/sample - loss: 0.3251 - acc: 0.8501\n",
      "Epoch 33/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.3143 - acc: 0.8589\n",
      "Epoch 34/100\n",
      "4862/4862 [==============================] - 0s 43us/sample - loss: 0.3112 - acc: 0.8622\n",
      "Epoch 35/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.3146 - acc: 0.8610\n",
      "Epoch 36/100\n",
      "4862/4862 [==============================] - 0s 48us/sample - loss: 0.3100 - acc: 0.8645\n",
      "Epoch 37/100\n",
      "4862/4862 [==============================] - 0s 59us/sample - loss: 0.3082 - acc: 0.8647\n",
      "Epoch 38/100\n",
      "4862/4862 [==============================] - 0s 75us/sample - loss: 0.2953 - acc: 0.8745\n",
      "Epoch 39/100\n",
      "4862/4862 [==============================] - 0s 73us/sample - loss: 0.2953 - acc: 0.8727\n",
      "Epoch 40/100\n",
      "4862/4862 [==============================] - 0s 68us/sample - loss: 0.2957 - acc: 0.8712\n",
      "Epoch 41/100\n",
      "4862/4862 [==============================] - 0s 48us/sample - loss: 0.2846 - acc: 0.8749\n",
      "Epoch 42/100\n",
      "4862/4862 [==============================] - 0s 57us/sample - loss: 0.2848 - acc: 0.8795\n",
      "Epoch 43/100\n",
      "4862/4862 [==============================] - 0s 60us/sample - loss: 0.2796 - acc: 0.8807\n",
      "Epoch 44/100\n",
      "4862/4862 [==============================] - 0s 74us/sample - loss: 0.2706 - acc: 0.8869\n",
      "Epoch 45/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.2700 - acc: 0.8852\n",
      "Epoch 46/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.2676 - acc: 0.8877\n",
      "Epoch 47/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.2643 - acc: 0.8916\n",
      "Epoch 48/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.2593 - acc: 0.8908\n",
      "Epoch 49/100\n",
      "4862/4862 [==============================] - 0s 58us/sample - loss: 0.2687 - acc: 0.8824\n",
      "Epoch 50/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.2524 - acc: 0.8965\n",
      "Epoch 51/100\n",
      "4862/4862 [==============================] - 0s 59us/sample - loss: 0.2515 - acc: 0.8963\n",
      "Epoch 52/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.2464 - acc: 0.8992\n",
      "Epoch 53/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.2498 - acc: 0.9048\n",
      "Epoch 54/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.2470 - acc: 0.8949\n",
      "Epoch 55/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.2404 - acc: 0.9044\n",
      "Epoch 56/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.2318 - acc: 0.9142\n",
      "Epoch 57/100\n",
      "4862/4862 [==============================] - 0s 73us/sample - loss: 0.2313 - acc: 0.9070\n",
      "Epoch 58/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.2281 - acc: 0.9114\n",
      "Epoch 59/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.2226 - acc: 0.9136\n",
      "Epoch 60/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.2173 - acc: 0.9159\n",
      "Epoch 61/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.2192 - acc: 0.9155\n",
      "Epoch 62/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.2204 - acc: 0.9142\n",
      "Epoch 63/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.2201 - acc: 0.9132\n",
      "Epoch 64/100\n",
      "4862/4862 [==============================] - 0s 67us/sample - loss: 0.2144 - acc: 0.9161\n",
      "Epoch 65/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.2160 - acc: 0.9144\n",
      "Epoch 66/100\n",
      "4862/4862 [==============================] - 0s 48us/sample - loss: 0.2004 - acc: 0.9264\n",
      "Epoch 67/100\n",
      "4862/4862 [==============================] - 0s 50us/sample - loss: 0.2054 - acc: 0.9202\n",
      "Epoch 68/100\n",
      "4862/4862 [==============================] - 0s 54us/sample - loss: 0.2024 - acc: 0.9237\n",
      "Epoch 69/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1976 - acc: 0.9253\n",
      "Epoch 70/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1897 - acc: 0.9288\n",
      "Epoch 71/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.1892 - acc: 0.9309\n",
      "Epoch 72/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1853 - acc: 0.9307\n",
      "Epoch 73/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1858 - acc: 0.9329\n",
      "Epoch 74/100\n",
      "4862/4862 [==============================] - 0s 72us/sample - loss: 0.1791 - acc: 0.9346\n",
      "Epoch 75/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.2201 - acc: 0.9163\n",
      "Epoch 76/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1840 - acc: 0.9332\n",
      "Epoch 77/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1776 - acc: 0.9393\n",
      "Epoch 78/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1805 - acc: 0.9338\n",
      "Epoch 79/100\n",
      "4862/4862 [==============================] - 0s 73us/sample - loss: 0.1686 - acc: 0.9414\n",
      "Epoch 80/100\n",
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.1706 - acc: 0.9352\n",
      "Epoch 81/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4862/4862 [==============================] - 0s 70us/sample - loss: 0.1772 - acc: 0.9362\n",
      "Epoch 82/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.1727 - acc: 0.9389\n",
      "Epoch 83/100\n",
      "4862/4862 [==============================] - 0s 47us/sample - loss: 0.1606 - acc: 0.9443\n",
      "Epoch 84/100\n",
      "4862/4862 [==============================] - 0s 44us/sample - loss: 0.1594 - acc: 0.9399\n",
      "Epoch 85/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.1587 - acc: 0.9451\n",
      "Epoch 86/100\n",
      "4862/4862 [==============================] - 0s 47us/sample - loss: 0.1552 - acc: 0.9469\n",
      "Epoch 87/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.1596 - acc: 0.9404\n",
      "Epoch 88/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.1504 - acc: 0.9494\n",
      "Epoch 89/100\n",
      "4862/4862 [==============================] - 0s 45us/sample - loss: 0.1543 - acc: 0.9461\n",
      "Epoch 90/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.1491 - acc: 0.9459\n",
      "Epoch 91/100\n",
      "4862/4862 [==============================] - 0s 46us/sample - loss: 0.1726 - acc: 0.9364\n",
      "Epoch 92/100\n",
      "4862/4862 [==============================] - 0s 44us/sample - loss: 0.1687 - acc: 0.9401\n",
      "Epoch 93/100\n",
      "4862/4862 [==============================] - 0s 62us/sample - loss: 0.1546 - acc: 0.9453\n",
      "Epoch 94/100\n",
      "4862/4862 [==============================] - 0s 67us/sample - loss: 0.1440 - acc: 0.9519\n",
      "Epoch 95/100\n",
      "4862/4862 [==============================] - 0s 48us/sample - loss: 0.1444 - acc: 0.9510\n",
      "Epoch 96/100\n",
      "4862/4862 [==============================] - 0s 73us/sample - loss: 0.1381 - acc: 0.9537\n",
      "Epoch 97/100\n",
      "4862/4862 [==============================] - 0s 68us/sample - loss: 0.1406 - acc: 0.9531\n",
      "Epoch 98/100\n",
      "4862/4862 [==============================] - 0s 71us/sample - loss: 0.1430 - acc: 0.9513\n",
      "Epoch 99/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.1432 - acc: 0.9517\n",
      "Epoch 100/100\n",
      "4862/4862 [==============================] - 0s 56us/sample - loss: 0.1290 - acc: 0.9574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4819105780>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(2048, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "model.compile(loss= 'sparse_categorical_crossentropy', optimizer= 'Adam' , metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model = LogisticRegression()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F-1 Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.nan_to_num(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarialize(vector) ->int:\n",
    "    if vector[0] < vector[1]:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n",
    "new_Y_pred = []\n",
    "for vector in Y_pred:\n",
    "    new_Y_pred.append(binarialize(vector))\n",
    "Y_pred = np.array(new_Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23376623376623376\n",
      "[[168 116]\n",
      " [  2  18]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f1_score(Y_test, Y_pred))\n",
    "cm = confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
